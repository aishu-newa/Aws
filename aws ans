** Introduction **

# 	What is cloud computing?
            Cloud computing is a technology that allows user to access and use computer resources like server, storage, db, networking, software, and more over the internet instead of relaying on local servers or personal devices, cloud computing uses a network of remote servers hosted on the internet to store, manage, and process data. This enables businesses and individuals to scale resources, access applications, and store data without the need for physical infrastructure. It’s a flexible and cost effective way to utilize computing power.


#  Deployment models in cloud?
            Here are 5 diff deployment models in cloud computing 
                            1.	Public cloud 
                            2.	Private cloud
                            3.	Hybrid cloud
                            4.	Community cloud
                            5.	Multi cloud
           These models provide options for organizations to select the cloud environment that aligns best with their specific needs and objectives.


# Service models in cloud?
       1  Iaas {infrastructure as a service} :
                     provides virtualized computing resource.
                     Users manage applications , data, and runtime environment.

       2  paas {platform as a service} :
                     offers a platform for application development and deployment.
                     Developers focus on coding, while platform handles infrastructure.

      3  saas { software as a service} : 
                     delivers ready to use software over the internet.
                     No need for local installation or maintenance.

These models represent varying levels of control and responsibility for users  in the cloud environment.



#  Architecture of cloud computing?
     It refers to the components and structure of a cloud environment, which enables the delivery of computing services over the internet. 
     Here are the kye components.

     1 Front end  : for users interact 
                   (like a website or app)
     2 Back end : for services are provided.
                  Including servers, storage, networking, security,and management.
    3 Middleware : connects front end and back end.
                  Allows them to work together.
    4 Cloud storage : for data is stored. 
                     Like a virtual hard drive.
    5 Deployment model : how cloud is set up. 
                     Public (for everyone) private (just for one  group) hybrid (mix). 
    6 Service models : types of services you can use. 
                    Iaas (like renting a computer), Paas (building app), Saas(using software online).
    These setup lets people use powerful computing without needing their own big computers.




#  AWS global infrastructure count? 
           In September 2021, amazon  web services (AWS) had a global infrastructure presence in 81 availability zone  across 25 geographic regions. 
           Note that there might have been expansions or changes since then. For the most up-to-date information. I recommend checking AWS’s official website. 


 #   Why do we have Regions?
           Regions in cloud computing exist to provide geographic locations where data centers are situated, enabling users to deploy resources closer to their users, comply with data sovereignty laws, 
           and enhance fault tolerance and resilience.



#  What is service? & what are resources?
                   1 Service :
                         A service is a "set of functions or tasks performed by a system, program, or application." 
                         It allows one component or system to request a specific function or action from another component, typically over a network.


                   2 Resources : "Resources refers to any data, information, or assets that are utilized or manipulated by a computer program or system." 
                                This can include files, db, memory, processing power, network connections, etc.  
                                In a broader context, resources can also refer to physical assets or commodities. 












** IAM { Identity and Access Management } **



#  How many resources do we have in IAM?
                AWS manages access to various AWS services and resources,
                including users, groups, roles, policies, and permissions.

 #  Deployment model in IAM?
         IAM is a cloud service that operates independently of specific deployment models, providing centralized control and management of  
         user access to resources within various cloud deployment models, including public, private, hybrid, and multi-cloud environments.
 
#   Identities in IAM ?
                   Identities are the "people or entities who are given permission to use an organization's resources." These can be employees, contractors, partners, customers, or anyone who needs access.
        Identities are usually "represented by user accounts or profiles in the IAM system." Each identity has certain characteristics like a username, password, email address, and roles or permissions that 
        determine what they can do and what they can access.


#  What is the IAM user?
                   "A user is someone who interacts with a computer system or network." In IAM, an IAM user is an entity that represents a person or service that interacts with a system or network using login credentials 
        such as a username and password. IAM users are created and managed within an IAM system


 	
 #   What is the IAM group?

             In IAM a "Group is a logical collection or grouping of IAM users". Think of it as a folder that contains multiple individuals 
             who share similar access permissions within a system.
                 Collection of Users
                 Simplified Management
                 Role-Based Access
                 Ease of Updates
                 Security and Control
              In essence, IAM groups are a way to simplify and streamline the management of permissions and access control by categorizing users based 
              on their roles or common access needs within an organization's digital infrastructure.
#   What is the IAM policy?
             IAM policies serve "as the rulebook or guidelines that dictate who has access to what resources and what actions they can perform," playing a crucial role 
             in maintaining security and access control within an organization's digital ecosystem.
 

#     What is the IAM Role?

               Roles in IAM offer a flexible and secure way to manage access control by defining and assigning granular permissions to various entities, ensuring that they have the right level of access required for specific tasks or functions.



#  Where do we attach Identity Based Policy?
                  "Role is like a set of permissions that say what someone can do and what they can access in an organization's systems."
        IAM roles are used to give specific privileges and access levels to people based on their jobs or responsibilities. These roles can be customized to fit the organization's needs.
        When someone assumes a role, they automatically get the permissions that come with that role. This means they can do the things and access the resources defined in the role, without needing individual 
        permissions. "IAM roles can be assigned to individual users, groups, or even to services."


#  Where do we attach Resource Based Policy?

Resource-based policies are attached directly to the resources they are meant to protect or control access to within an IAM 


#  Can we be able to create Policy via json code?
 
  It's possible to create policies using JSON “JavaScript Object Notation” 
Policies defined in JSON format can contain rules, permissions, conditions, and other criteria that define access control or govern behavior within a system or application.
For example, in the context of access control, a policy might specify which users or groups have permission to perform certain actions on a system or resource.

  

#  What is dominator policy?

         A dominator policy, in the context of access control, "refers to a policy that grants broader or higher-level permissions than other policies" within a given access control scheme, 
          effectively having overriding authority or control over subordinate policies for certain resources or actions.

 #  What is ARN? What are the fields in ARN?

         ARN stands for Amazon Resource Name. It is a unique identifier assigned to resources in the Amazon Web Services (AWS) ecosystem. 
          The fields in an ARN typically include the 
                       1 Service name
                       2 Region
                       3 Account ID
                      4 Resource type 
                        5 Resource-specific information.

     To identify resources and has the following fields:
         arn:partition:service:region:account-id:resource   
    example: AWS S3 bucket ARN look like : arn:aws:s3:::my_bucket


 #   How many types of ARN partition?




 #   What are tags?

In computing and specifically within cloud services like AWS, Azure, and Google Cloud, “tags” are metadata elements used to label and organize resources. 
“Tags consist of key-value pairs that provide additional information about a resource, making it easier to manage, search for, identify, and categorize resources within a system or service.”
Tags are metadata labels used to annotate{like a comment or note} and categorize AWS resources for organizational and management purposes.





	**  S3 {simple storage service}  **


#  Difference between Block storage and Object storage?

   BLOCK STORAGE = Block storage manages data in fixed-sized chunks, suitable for operating systems and databases.
   OBJECT STORAGE = object storage stores data as objects with metadata, ideal for scalable and unstructured                                                                                       


 # Difference between static and dynamic website?
           STATIC WEBSITE = A static website contains web pages with fixed content, displaying the same information to all visitors, typically without dynamic 
                             or interactive elements. 
           DYNAMIC WEBSITE = A dynamic website generates web pages with content that can change based on user interactions, database queries, 
                              or other external factors, offering personalized and interactive experiences.

           Examples:    STATIC WEBSTIE = Basic informational sites, portfolios  
                        DYNAMIC WEBSITE = E-commerce sites, social media platforms        |



#   What are the naming rules?
             In AWS S3, naming rules require bucket names to be globally unique, between 3 and 63 characters long, lowercase, and containing no uppercase letters or underscores.

#   What is the major resources of S3 Bucket?
            The primary resource in an AWS S3 (Simple Storage Service) bucket is the object, 
            which refers to the data (files, documents, images, etc.) stored within the bucket.



#   Why do we need to host static website instead of dynamic website?

            Static websites are often hosted because they are simpler, faster to load, cost-effective, and suitable for content that doesn't change frequently, whereas dynamic websites are more complex, 
            require server-side processing,and are better for interactive or frequently updated content.


#   What are the objects and types of objects that we are uploading into the s3 bucket ?
                        OBJECT  in AWS S3 represent the fundamental entities stored within buckets, encompassing files, data,
                                  or any other type of content, paired with metadata.
                        TYPE OF OBJECT 

                        1. Standard Object: Regular file uploads.
                        2. Multipart Object: Large files split into smaller parts for efficient uploading.
                        3. Versioned Object: Files with multiple versions tracked over time.
                        4. Lifecycle Object: Objects with predefined rules for transitions or expiration.
                        5. Encrypted Object: Files encrypted for added security.
                        6. Replicated Object: Objects copied to different regions for redundancy.


#   Why is MFA delete important in s3 bucket object level?
       MFA stands for “Multi-Factor Authentication” MFA delete is important in S3 bucket object-level security because it adds an extra layer of protection by requiring an additional authentication method 
       such as a physical token or authenticator app before allowing the deletion of objects, thereby reducing the risk of accidental or unauthorized data removal.


#    What is s3 multipart upload?

            S3 multipart upload is a feature that allows large objects to be uploaded in smaller parts for increased efficiency, resumability, and faster uploads.

#    What are the storage classes in amazon s3? 
            The storage classes in Amazon S3 are:
                                    1 Standard,
                                   2 Intelligent-Tiering,
                                   3 Standard-IA (Infrequent Access), 
                                   4 One Zone-IA,             
                                   5 Glacier, 
                                   6 Glacier Deep Archive, 
                                   
#     What is ACL? Why do we need ACL?

            ACL stands for Access Control List. It's a security measure used in computer systems and networks to manage permissions and control access to resources
            like files, folders, or networks. Essentially, an 
            ACL is a list of rules or settings that determines who can access what resources and what actions they're allowed to perform, 
            such as reading, writing, or executing files.
            It helps in regulating and securing access rights for different users or groups within a system or network.
            ACL are used for several reasons, primarily to control and manage who can access what resources in a system or network.



#    What is life cycle policy? Why do we need to use the life cycle rule?
            A lifecycle policy is a rule that automates the management of objects by defining actions like transitioning to different storage classes or 
             deleting objects based on specified criteria over time.

       Life cycle rules are necessary for several reasons like:

                        1.DataManagement:
                        2.Compliance
                        3.Cost Optimization:
                        4. Security :
Overall, life cycle rules help organizations manage their data more effectively, comply with regulations, reduce costs, and enhance security by defining a
          clear path for how data should be handled from creation to disposal.

#    How can we make our bucket public?
     To make S3 bucket public :

1.Access S3 Service:
2.Select the Bucket: 
3.Permissions Tab:
4.Bucket Policy: 
5.Add Bucket Policy for Public Access:
6.Save Changes: 
7.Confirmation:


#     How can we give public access to our bucket?

            To give public access to S3 bucket, you can use bucket policies or access control lists (ACLs). 
            Here's a guide using bucket policies via the AWS Management Console:

            1.Sign in to AWS Console:

            2.Access S3 Service:
   
            3.Select the Bucket:

            4.Permissions Tab:

            5.Bucket Policy:

            6.Add Bucket Policy for Public Access:
   Lets have an example bucket policy that grants public read access to objects in your bucket
   
  * In json code*

   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Effect": "Allow",
               "Principal": "*",
               "Action": "s3:GetObject",
               "Resource": "arn:aws:s3:::publicbucket/*"
           }
       ]
   }
   
   these “publicbucket” is public now.

7.Save Changes:
   After pasting the policy, click on "Save" or "Save changes" to apply the bucket policy.

This policy grants read access to all users ("Principal": "*") to retrieve objects within the specified bucket.

.
 	AWS pricing factor of the s3 service?
AWS S3 pricing is based on a simple factors that include the following:

1.Storage Usage: You are charged for the amount of data you store in S3. This is measured in gigabytes (GB) or terabytes (TB) per month. “There are different storage classes with varying prices based on the access frequency and retrieval options”.

2.Requests [a library for making HTTP requests]: AWS charges for the number of requests made to S3, such as when you upload, download, or copy files. Requests can include GET (read), PUT (write), COPY, LIST, DELETE operations, etc.

3.Data Transfer: The cost of transferring data out of S3 to the internet or other AWS regions incurs charges. Incoming data transfer is generally free, but outbound data transfer to the internet or between AWS regions has associated costs.

4.Storage Management Features: Additional features like versioning, cross-region replication, or data lifecycle management might have associated costs.

5.Storage Class: S3 offers different storage classes with different prices. For instance, Standard storage is costlier but offers immediate access, while Glacier or Infrequent Access (IA) storage classes are cheaper but might have retrieval fees or slower access times.

  AWS S3 pricing can vary based on regions, storage classes, and any additional features you use. It's essential to check the AWS pricing page for the most up-to-date and detailed information. Usage of each of these factors contributes to your overall bill for using the AWS S3 service.

 	How can we make our object public?

Making an object public involves adjusting its access permissions so that it can be accessed by anyone on the internet. In cloud storage services like Amazon S3, you can do this by modifying the object's access control settings:

1.Access your storage service : Sign in to your cloud storage account (e.g., Amazon S3).

2.Locate the object: Find the specific file or object that you want to make public within your storage buckets or folders.

3.Access object settings: Navigate to the object's settings or properties, often found by right-clicking on the object or through a menu option.

4.Modify permissions: Look for an option related to permissions, access control, or sharing. Change the settings to allow public access by adjusting the object's permissions to "public" or by setting it to be accessible to everyone.

5.Save changes: After adjusting the permissions to allow public access, save the changes or apply the new settings.

 After doing these steps, the specific object will be accessible to anyone who has the object's URL or link. Making an object public means it can be viewed or downloaded by anyone on the internet, so be cautious and ensure that you intend to share the content publicly.
 	How can we configure the static website logs in s3?

Configuring static website logs in S3 involves enabling server access logging for your S3 bucket, which helps you track and analyze access to your static website:

1.Sign in to AWS Console.
2. Access S3 Service: Once logged in, find and select the "S3" service from the list of AWS services.

3.Select the Bucket: Click on the name of the S3 bucket that hosts your static website.

4.Properties Tab: Within the selected bucket, navigate to the "Properties" tab located in the top panel.

5.Static Website Hosting: If your bucket is configured to host a static website, ensure that it's properly set up for static website hosting. Note down the endpoint URL of your static website.

6.Server Access Logging: Scroll down to the "Server access logging" section.

7.Enable Logging : Click on "Edit" or "Enable logging." Then choose an existing bucket where you want the logs to be stored or create a new bucket to store the logs.

8.Logging Options: Configure the logging options, including the target bucket, target prefix (optional folder within the bucket), and the log file format.

9.Save Changes: After configuring the settings, save the changes by clicking "Save" or "Save changes."

Once server access logging is enabled, S3 will start generating access logs that provide information about requests made to your static website, including details such as the requester's IP address, request time, HTTP status codes, etc. You can then analyze these logs using various tools or services to gain insights into your website's traffic patterns and usage.

{Please note that enabling logging can incur additional storage costs in the bucket where the logs are stored. Ensure you understand the potential cost implications and regularly review and manage your logs to avoid unnecessary charges.}
 	What is CORS?

CORS stands for Cross-Origin Resource Sharing. It's a security feature implemented by web browsers that controls which resources (like fonts, JavaScript, etc.) loaded from one domain can be requested by a web page from a different domain. 

When a web page makes a request to a different domain (origin) for resources like data or assets, CORS acts as a security measure to prevent certain types of attacks, like cross-site request forgery. It requires the server hosting the requested resource to include specific HTTP headers that permit or restrict cross-origin requests from web pages on other domains.

CORS policies are typically set on the server-side to specify which origins, HTTP methods, and headers are allowed when making requests to that server. This helps ensure better security by controlling access to resources across different origins while allowing legitimate cross-origin requests to go through.
 	 What is s3 inventory?

1.Inventory List: S3 Inventory is a feature provided by Amazon Web Services (AWS) for Amazon S3 (Simple Storage Service). It generates a list of your objects (files) and their metadata within an S3 bucket.

2.Scheduled Reports: It creates reports at scheduled intervals (daily or weekly) or on-demand, detailing the objects' metadata, such as size, storage class, encryption status, etc.

3.Helps Analysis: S3 Inventory makes it easier to manage and analyze large amounts of data stored in S3 buckets by providing a detailed list of objects, which can be used for compliance, auditing, and analytics purposes.

4.Customizable: You can choose the specific data fields (metadata) you want in the inventory report, tailoring it to your specific needs, which helps in optimizing storage and access patterns.

5.Cost-Effective: While there's a small fee for generating S3 Inventory reports, it can save time and resources by providing efficient access to detailed object information without needing to scan the entire bucket repeatedly.

6.Integration: S3 Inventory can be integrated with other AWS services or third-party tools for further analysis, management, and data lifecycle policies.

S3 Inventory simplifies the process of managing and analyzing the vast amount of data stored in Amazon S3 buckets by providing detailed object metadata in scheduled or on-demand reports.
 	What does it mean by Requester pays?

“Requester pays” in the context of Amazon S3 means that when someone who is not the bucket owner accesses data stored in an S3 bucket, they are responsible for the data transfer and operational costs associated with that access. In simpler terms, if you retrieve or download data from someone else's S3 bucket, you (the requester) pay for the data transfer and any applicable charges, instead of the bucket owner covering those expenses. This setup allows bucket owners to share access to their data while offloading the costs to those who access it.
 	What is the secondary word to transfer acceleration? Why do we need to use this transfer acceleration?

The secondary word for “Transfer Acceleration” in the context of Amazon S3 is “fast transfer.”
Transfer Acceleration in Amazon S3 is used to speed up the process of uploading data to an S3 bucket by optimizing data transfer paths, especially for users located at a distance from the AWS region where the bucket is stored.
1  Faster Uploads: The Amazon CloudFront network, which uses a globally distributed network of edge locations. This helps s
2 Geographical Distance: If there's a significant geographic distance between the user (requester) and the AWS S3 bucket's region, standard uploads might be slower due to network latency. Transfer Acceleration routes the data through Amazon's optimized network paths, reducing latency and speeding up transfers.

3 Consistent Performance: It provides more consistent performance regardless of the client's location.
4   Alternative to Direct Uploads: In cases where direct uploads to the S3 bucket might be slow due to network constraints or long distances, Transfer Acceleration can offer a quicker alternative.
5  Ease of Use: It's simple to enable Transfer Acceleration for an S3 bucket through AWS Management Console, SDKs, or APIs without requiring any changes to the application code.



	AWS Cloud Trail 



 	What is a cloud trail?
 	Why do we use trails, what is the exact purpose of enabling the trail in cloud production accounts?
 	Explain how we can create a trail in aws cloud trail?



 	
